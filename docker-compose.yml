services:
  vector:
    build: ./vector
    mem_limit: 4g
    environment:
      - VECTOR_CONFIG=/etc/vector/vector.toml
    volumes:
      - ./temp_logs:/data/logs:ro
      - ./clean:/data/clean
    restart: on-failure
    healthcheck:
      test: ["CMD", "test", "-f", "/data/clean/parsed.jsonl"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 300s

  ollama:
    image: ollama/ollama@sha256:c76340da9ed9906b01ed5462ca966b608de93a76c1fbc01123d07034a99f699b
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD","curl", "-f", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 60s
      retries: 10
      start_period: 120s #wait 2 minutes before first check

  api:
    build: ./api
    environment:
      - OLLAMA_URL=http://ollama:11434
      - MODEL=phi3
      - INDEX_DIR=/app/index
      - LOG_JSONL=/app/clean/parsed.jsonl
      - DOC_DIR=/app/docs
      - EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
    volumes:
      - ./clean:/app/clean:ro
      - ./docs:/app/docs:ro
      - ./index:/app/index
    ports:
      - "8000:8000"
    depends_on:
      vector:
        condition: service_healthy
#      ollama:
#        condition: service_healthy
    restart: unless-stopped

  indexer:
    build: ./api
    command: ["python", "build_index.py"]
    environment:
      - INDEX_DIR=/app/index
      - LOG_JSONL=/app/clean/parsed.jsonl
      - DOC_DIR=/app/docs
      - EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
    volumes:
      - ./clean:/app/clean:ro
      - ./docs:/app/docs:ro
      - ./index:/app/index
    depends_on:
      vector:
        condition: service_healthy
    restart: "no"

volumes:
  ollama_models:
